info: cargo-llvm-cov currently setting cfg(coverage); you can opt-out it by passing --no-cfg-coverage
   Compiling gpusorted_map v0.1.0 (/Users/kentb/Dropbox/Mac/Documents/GitHub/GPUSortedMap)
warning: unused imports: `readback_single` and `readback_vec`
 --> src/gpu_array.rs:4:48
  |
4 | ...e_buffer_with_data, readback_single, readback_vec};
  |                        ^^^^^^^^^^^^^^^  ^^^^^^^^^^^^
  |
  = note: `#[warn(unused_imports)]` (part of `#[warn(unused)]`) on by default
warning: `gpusorted_map` (lib) generated 1 warning (run `cargo fix --lib -p gpusorted_map` to apply 1 suggestion)
warning: `gpusorted_map` (lib test) generated 1 warning (1 duplicate)
    Finished `test` profile [unoptimized + debuginfo] target(s) in 0.37s
     Running unittests src/lib.rs (target/llvm-cov-target/debug/deps/gpusorted_map-5ae2638baaab8371)
running 10 tests
test gpu_array::tests::creates_with_capacity ... ok
test gpu_array::tests::update_len_clamps_and_updates_meta ... ok
test gpu_array::tests::write_persists_data ... ok
test tests::put_rejects_tombstone_value ... ok
test tests::creates_gpu_sorted_map ... ok
test tests::put_then_get ... ok
test tests::single_put_then_get ... ok
test tests::delete_single_key ... ok
test tests::single_get_missing_key ... ok
test tests::bulk_delete_clears_values ... ok
test result: ok. 10 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.07s
     Running unittests src/bin/perf.rs (target/llvm-cov-target/debug/deps/perf-39119568b2a16554)
running 0 tests
test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s
warning: 1 functions have mismatched data
/Users/kentb/Dropbox/Mac/Documents/GitHub/GPUSortedMap/src/bin/perf.rs:
    1|       |use gpusorted_map::{GpuSortedMap, KvEntry};
    2|       |use rand::rngs::StdRng;
    3|       |use rand::{RngCore, SeedableRng};
    4|       |use std::collections::BTreeMap;
    5|       |use std::fs::{self, File};
    6|       |use std::io::Write;
    7|       |use std::path::PathBuf;
    8|       |use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
    9|       |
   10|      0|fn main() {
   11|      0|    let sizes = [1_usize, 10, 100, 1_000, 10_000, 100_000, 1_000_000];
   12|      0|    let mut rng = StdRng::seed_from_u64(0x5eed);
   13|       |
   14|      0|    let mut rows = Vec::new();
   15|      0|    for size in sizes {
   16|      0|        let keys: Vec<u32> = (0..size).map(|_| rng.next_u32()).collect();
   17|      0|        let entries: Vec<KvEntry> = keys
   18|      0|            .iter()
   19|      0|            .map(|&key| KvEntry {
   20|      0|                key,
   21|      0|                value: key.wrapping_mul(31),
   22|      0|            })
   23|      0|            .collect();
   24|       |
   25|      0|        let (btree_put, btree_get) = bench_btree(&keys);
   26|      0|        let (gpu_put, gpu_get) = bench_gpu(&entries, &keys);
   27|       |
   28|      0|        rows.push((
   29|      0|            size,
   30|      0|            btree_put,
   31|      0|            btree_get,
   32|      0|            gpu_put,
   33|      0|            gpu_get,
   34|      0|        ));
   35|       |    }
   36|       |
   37|      0|    let commit = git_head().unwrap_or_else(|| "unknown".to_string());
   38|      0|    let stamp = unix_timestamp();
   39|      0|    let dir = PathBuf::from("perf");
   40|      0|    if let Err(err) = fs::create_dir_all(&dir) {
   41|      0|        eprintln!("failed to create perf dir: {err}");
   42|      0|        std::process::exit(1);
   43|      0|    }
   44|       |
   45|      0|    let filename = format!("{commit}_{stamp}.csv");
   46|      0|    let path = dir.join(filename);
   47|      0|    let mut file = File::create(&path).expect("failed to create perf output file");
   48|      0|    writeln!(
   49|      0|        file,
   50|       |        "size,btree_put_ms,btree_get_ms,gpu_put_ms,gpu_get_ms"
   51|       |    )
   52|      0|    .expect("failed to write header");
   53|       |
   54|      0|    for (size, btree_put, btree_get, gpu_put, gpu_get) in rows {
   55|      0|        writeln!(
   56|      0|            file,
   57|      0|            "{},{:.3},{:.3},{:.3},{:.3}",
   58|      0|            size,
   59|      0|            duration_ms(btree_put),
   60|      0|            duration_ms(btree_get),
   61|      0|            duration_ms(gpu_put),
   62|      0|            duration_ms(gpu_get)
   63|      0|        )
   64|      0|        .expect("failed to write row");
   65|      0|    }
   66|       |
   67|      0|    println!("wrote perf results to {}", path.display());
   68|      0|}
   69|       |
   70|      0|fn bench_btree(keys: &[u32]) -> (Duration, Duration) {
   71|      0|    let mut map = BTreeMap::new();
   72|      0|    let start = Instant::now();
   73|      0|    for &key in keys {
   74|      0|        map.insert(key, key.wrapping_mul(31));
   75|      0|    }
   76|      0|    let put = start.elapsed();
   77|       |
   78|      0|    let start = Instant::now();
   79|      0|    let mut sink = 0_u64;
   80|      0|    for &key in keys {
   81|      0|        if let Some(value) = map.get(&key) {
   82|      0|            sink = sink.wrapping_add(*value as u64);
   83|      0|        }
   84|       |    }
   85|      0|    std::hint::black_box(sink);
   86|      0|    (put, start.elapsed())
   87|      0|}
   88|       |
   89|      0|fn bench_gpu(entries: &[KvEntry], keys: &[u32]) -> (Duration, Duration) {
   90|      0|    let mut map = pollster::block_on(GpuSortedMap::new((entries.len() * 2) as u32))
   91|      0|        .expect("failed to init GPU map");
   92|       |
   93|      0|    let start = Instant::now();
   94|      0|    map.bulk_put(entries).expect("bulk_put failed");
   95|      0|    let put = start.elapsed();
   96|       |
   97|      0|    let start = Instant::now();
   98|      0|    let values = map.bulk_get(keys);
   99|      0|    std::hint::black_box(values);
  100|      0|    (put, start.elapsed())
  101|      0|}
  102|       |
  103|      0|fn git_head() -> Option<String> {
  104|      0|    let output = std::process::Command::new("git")
  105|      0|        .args(["rev-parse", "HEAD"])
  106|      0|        .output()
  107|      0|        .ok()?;
  108|      0|    if !output.status.success() {
  109|      0|        return None;
  110|      0|    }
  111|      0|    let mut head = String::from_utf8_lossy(&output.stdout).to_string();
  112|      0|    head.truncate(head.trim_end().len());
  113|      0|    Some(head.trim().to_string())
  114|      0|}
  115|       |
  116|      0|fn unix_timestamp() -> u64 {
  117|      0|    SystemTime::now()
  118|      0|        .duration_since(UNIX_EPOCH)
  119|      0|        .unwrap_or_else(|_| Duration::from_secs(0))
  120|      0|        .as_secs()
  121|      0|}
  122|       |
  123|      0|fn duration_ms(duration: Duration) -> f64 {
  124|      0|    duration.as_secs_f64() * 1_000.0
  125|      0|}
/Users/kentb/Dropbox/Mac/Documents/GitHub/GPUSortedMap/src/gpu_array.rs:
    1|       |use bytemuck::{Pod, Zeroable};
    2|       |use std::marker::PhantomData;
    3|       |
    4|       |use crate::pipeline::{create_buffer_with_data, readback_single, readback_vec};
    5|       |
    6|       |#[repr(C)]
    7|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
    8|       |pub struct SlabMeta {
    9|       |    pub len: u32,
   10|       |    pub capacity: u32,
   11|       |    pub _pad: [u32; 2],
   12|       |}
   13|       |
   14|       |pub struct GpuArray<T: Pod> {
   15|       |    buffer: wgpu::Buffer,
   16|       |    meta_buffer: wgpu::Buffer,
   17|       |    capacity: u32,
   18|       |    len: u32,
   19|       |    _marker: PhantomData<T>,
   20|       |}
   21|       |
   22|       |impl<T: Pod> GpuArray<T> {
   23|     24|    pub fn new(
   24|     24|        device: &wgpu::Device,
   25|     24|        capacity: u32,
   26|     24|        buffer_usage: wgpu::BufferUsages,
   27|     24|        label: &str,
   28|     24|    ) -> Self {
   29|     24|        let size = (capacity as u64) * std::mem::size_of::<T>() as u64;
   30|     24|        let buffer = device.create_buffer(&wgpu::BufferDescriptor {
   31|     24|            label: Some(label),
   32|     24|            size,
   33|     24|            usage: buffer_usage,
   34|     24|            mapped_at_creation: false,
   35|     24|        });
   36|       |
   37|     24|        let meta = SlabMeta {
   38|     24|            len: 0,
   39|     24|            capacity,
   40|     24|            _pad: [0; 2],
   41|     24|        };
   42|     24|        let meta_buffer = create_buffer_with_data(
   43|     24|            device,
   44|     24|            "slab-meta-buffer",
   45|     24|            wgpu::BufferUsages::UNIFORM
   46|     24|                | wgpu::BufferUsages::COPY_DST
   47|     24|                | wgpu::BufferUsages::COPY_SRC,
   48|     24|            &[meta],
   49|       |        );
   50|       |
   51|     24|        Self {
   52|     24|            buffer,
   53|     24|            meta_buffer,
   54|     24|            capacity,
   55|     24|            len: 0,
   56|     24|            _marker: PhantomData,
   57|     24|        }
   58|     24|    }
   59|       |
   60|     36|    pub fn buffer(&self) -> &wgpu::Buffer {
   61|     36|        &self.buffer
   62|     36|    }
   63|       |
   64|     12|    pub fn meta_buffer(&self) -> &wgpu::Buffer {
   65|     12|        &self.meta_buffer
   66|     12|    }
   67|       |
   68|     13|    pub fn capacity(&self) -> u32 {
   69|     13|        self.capacity
   70|     13|    }
   71|       |
   72|      6|    pub fn len(&self) -> u32 {
   73|      6|        self.len
   74|      6|    }
   75|       |
   76|      5|    pub fn update_len(&mut self, queue: &wgpu::Queue, new_len: u32) {
   77|      5|        self.len = new_len.min(self.capacity);
   78|      5|        let meta = SlabMeta {
   79|      5|            len: self.len,
   80|      5|            capacity: self.capacity,
   81|      5|            _pad: [0; 2],
   82|      5|        };
   83|      5|        queue.write_buffer(&self.meta_buffer, 0, bytemuck::bytes_of(&meta));
   84|      5|    }
   85|       |
   86|      5|    pub fn write(&self, queue: &wgpu::Queue, data: &[T]) {
   87|      5|        if data.is_empty() {
   88|      0|            return;
   89|      5|        }
   90|      5|        queue.write_buffer(&self.buffer, 0, bytemuck::cast_slice(data));
   91|      5|    }
   92|       |}
   93|       |
   94|       |pub struct GpuStorage<T: Pod> {
   95|       |    buffer: wgpu::Buffer,
   96|       |    _marker: PhantomData<T>,
   97|       |}
   98|       |
   99|       |impl<T: Pod> GpuStorage<T> {
  100|      7|    pub fn new(device: &wgpu::Device, usage: wgpu::BufferUsages, label: &str) -> Self {
  101|      7|        let size = std::mem::size_of::<T>() as u64;
  102|      7|        let buffer = device.create_buffer(&wgpu::BufferDescriptor {
  103|      7|            label: Some(label),
  104|      7|            size,
  105|      7|            usage,
  106|      7|            mapped_at_creation: false,
  107|      7|        });
  108|      7|        Self {
  109|      7|            buffer,
  110|      7|            _marker: PhantomData,
  111|      7|        }
  112|      7|    }
  113|       |
  114|     16|    pub fn buffer(&self) -> &wgpu::Buffer {
  115|     16|        &self.buffer
  116|     16|    }
  117|       |}
  118|       |
  119|       |#[cfg(test)]
  120|       |mod tests {
  121|       |    use super::{GpuArray, SlabMeta};
  122|       |
  123|      3|    async fn create_device_queue() -> (wgpu::Device, wgpu::Queue) {
  124|      3|        let instance = wgpu::Instance::new(wgpu::InstanceDescriptor {
  125|      3|            backends: wgpu::Backends::all(),
  126|      3|            ..Default::default()
  127|      3|        });
  128|      3|        let adapter = instance
  129|      3|            .request_adapter(&wgpu::RequestAdapterOptions {
  130|      3|                power_preference: wgpu::PowerPreference::LowPower,
  131|      3|                compatible_surface: None,
  132|      3|                force_fallback_adapter: false,
  133|      3|            })
  134|      3|            .await
  135|      3|            .expect("no suitable GPU adapters found");
  136|      3|        adapter
  137|      3|            .request_device(
  138|      3|                &wgpu::DeviceDescriptor {
  139|      3|                    label: Some("gpu-array-test-device"),
  140|      3|                    required_features: wgpu::Features::empty(),
  141|      3|                    required_limits: wgpu::Limits::default(),
  142|      3|                },
  143|      3|                None,
  144|      3|            )
  145|      3|            .await
  146|      3|            .expect("failed to request device")
  147|      3|    }
  148|       |
  149|      2|    fn readback_vec<T: bytemuck::Pod>(
  150|      2|        device: &wgpu::Device,
  151|      2|        queue: &wgpu::Queue,
  152|      2|        source: &wgpu::Buffer,
  153|      2|        byte_len: u64,
  154|      2|    ) -> Vec<T> {
  155|      2|        let readback = device.create_buffer(&wgpu::BufferDescriptor {
  156|      2|            label: Some("gpu-array-readback"),
  157|      2|            size: byte_len,
  158|      2|            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
  159|      2|            mapped_at_creation: false,
  160|      2|        });
  161|       |
  162|      2|        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  163|      2|            label: Some("gpu-array-readback-encoder"),
  164|      2|        });
  165|      2|        encoder.copy_buffer_to_buffer(source, 0, &readback, 0, byte_len);
  166|      2|        queue.submit(Some(encoder.finish()));
  167|       |
  168|      2|        crate::pipeline::readback_vec::<T>(device, &readback)
  169|      2|    }
  170|       |
  171|       |    #[test]
  172|      1|    fn creates_with_capacity() {
  173|      1|        let (device, _queue) = pollster::block_on(create_device_queue());
  174|      1|        let array = GpuArray::<u32>::new(
  175|      1|            &device,
  176|       |            4,
  177|      1|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
  178|      1|            "test-buffer",
  179|       |        );
  180|      1|        assert_eq!(array.capacity(), 4);
  181|      1|        assert_eq!(array.len(), 0);
  182|      1|    }
  183|       |
  184|       |    #[test]
  185|      1|    fn update_len_clamps_and_updates_meta() {
  186|      1|        let (device, queue) = pollster::block_on(create_device_queue());
  187|      1|        let mut array = GpuArray::<u32>::new(
  188|      1|            &device,
  189|       |            4,
  190|      1|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
  191|      1|            "test-buffer",
  192|       |        );
  193|       |
  194|      1|        array.update_len(&queue, 10);
  195|      1|        assert_eq!(array.len(), 4);
  196|       |
  197|      1|        let meta = readback_vec::<SlabMeta>(
  198|      1|            &device,
  199|      1|            &queue,
  200|      1|            array.meta_buffer(),
  201|      1|            std::mem::size_of::<SlabMeta>() as u64,
  202|       |        );
  203|      1|        assert_eq!(meta.len(), 1);
  204|      1|        assert_eq!(meta[0].len, 4);
  205|      1|        assert_eq!(meta[0].capacity, 4);
  206|      1|    }
  207|       |
  208|       |    #[test]
  209|      1|    fn write_persists_data() {
  210|      1|        let (device, queue) = pollster::block_on(create_device_queue());
  211|      1|        let array = GpuArray::<u32>::new(
  212|      1|            &device,
  213|       |            4,
  214|      1|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::COPY_SRC,
  215|      1|            "test-buffer",
  216|       |        );
  217|       |
  218|      1|        let data = [10_u32, 20, 30, 40];
  219|      1|        array.write(&queue, &data);
  220|       |
  221|      1|        let readback = readback_vec::<u32>(
  222|      1|            &device,
  223|      1|            &queue,
  224|      1|            array.buffer(),
  225|      1|            (data.len() * std::mem::size_of::<u32>()) as u64,
  226|       |        );
  227|      1|        assert_eq!(readback, data);
  228|      1|    }
  229|       |}
/Users/kentb/Dropbox/Mac/Documents/GitHub/GPUSortedMap/src/lib.rs:
    1|       |mod gpu_array;
    2|       |mod pipeline;
    3|       |
    4|       |use bytemuck::{Pod, Zeroable};
    5|       |use std::sync::Arc;
    6|       |
    7|       |use crate::gpu_array::{GpuArray, GpuStorage};
    8|       |use crate::pipeline::{BulkDeletePipeline, BulkGetPipeline, BulkPutPipeline, MergeMeta, TOMBSTONE_VALUE};
    9|       |
   10|       |#[repr(C)]
   11|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   12|       |pub struct KvEntry {
   13|       |    pub key: u32,
   14|       |    pub value: u32,
   15|       |}
   16|       |
   17|       |pub struct GpuSortedMap {
   18|       |    queue: Arc<wgpu::Queue>,
   19|       |    slab: GpuArray<KvEntry>,
   20|       |    input: GpuArray<KvEntry>,
   21|       |    merge: GpuArray<KvEntry>,
   22|       |    merge_meta: GpuStorage<MergeMeta>,
   23|       |    bulk_get: BulkGetPipeline,
   24|       |    bulk_delete: BulkDeletePipeline,
   25|       |    bulk_put: BulkPutPipeline,
   26|       |}
   27|       |
   28|       |impl GpuSortedMap {
   29|      7|    pub async fn new(capacity: u32) -> Result<Self, wgpu::RequestDeviceError> {
   30|      7|        let instance = wgpu::Instance::new(wgpu::InstanceDescriptor {
   31|      7|            backends: wgpu::Backends::all(),
   32|      7|            ..Default::default()
   33|      7|        });
   34|      7|        let adapter = instance
   35|      7|            .request_adapter(&wgpu::RequestAdapterOptions {
   36|      7|                power_preference: wgpu::PowerPreference::HighPerformance,
   37|      7|                compatible_surface: None,
   38|      7|                force_fallback_adapter: false,
   39|      7|            })
   40|      7|            .await
   41|      7|            .expect("no suitable GPU adapters found");
   42|       |
   43|      7|        let (device, queue) = adapter
   44|      7|            .request_device(
   45|      7|                &wgpu::DeviceDescriptor {
   46|      7|                    label: Some("gpu-sorted-map-device"),
   47|      7|                    required_features: wgpu::Features::empty(),
   48|      7|                    required_limits: wgpu::Limits::default(),
   49|      7|                },
   50|      7|                None,
   51|      7|            )
   52|      7|            .await?;
                                ^0
   53|      7|        let device = Arc::new(device);
   54|      7|        let queue = Arc::new(queue);
   55|       |
   56|      7|        let slab = GpuArray::new(
   57|      7|            &device,
   58|      7|            capacity,
   59|      7|            wgpu::BufferUsages::STORAGE
   60|      7|                | wgpu::BufferUsages::COPY_DST
   61|      7|                | wgpu::BufferUsages::COPY_SRC,
   62|      7|            "slab-buffer",
   63|       |        );
   64|       |
   65|      7|        let input = GpuArray::new(
   66|      7|            &device,
   67|      7|            capacity,
   68|      7|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
   69|      7|            "input-buffer",
   70|       |        );
   71|       |
   72|      7|        let merge = GpuArray::new(
   73|      7|            &device,
   74|      7|            capacity,
   75|      7|            wgpu::BufferUsages::STORAGE
   76|      7|                | wgpu::BufferUsages::COPY_DST
   77|      7|                | wgpu::BufferUsages::COPY_SRC,
   78|      7|            "merge-buffer",
   79|       |        );
   80|       |
   81|      7|        let merge_meta = GpuStorage::new(
   82|      7|            &device,
   83|      7|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC,
   84|      7|            "merge-meta-buffer",
   85|       |        );
   86|       |
   87|      7|        let bulk_get = BulkGetPipeline::new(Arc::clone(&device), Arc::clone(&queue));
   88|      7|        let bulk_delete = BulkDeletePipeline::new(Arc::clone(&device), Arc::clone(&queue));
   89|      7|        let bulk_put = BulkPutPipeline::new(Arc::clone(&device), Arc::clone(&queue));
   90|       |
   91|      7|        Ok(Self {
   92|      7|            queue,
   93|      7|            slab,
   94|      7|            input,
   95|      7|            merge,
   96|      7|            merge_meta,
   97|      7|            bulk_get,
   98|      7|            bulk_delete,
   99|      7|            bulk_put,
  100|      7|        })
  101|      7|    }
  102|       |
  103|      5|    pub fn bulk_get(&self, keys: &[u32]) -> Vec<Option<u32>> {
  104|      5|        self.bulk_get.execute(&self.slab, keys)
  105|      5|    }
  106|       |
  107|      5|    pub fn bulk_put(&mut self, entries: &[KvEntry]) -> Result<(), GpuMapError> {
  108|      5|        if entries.is_empty() {
  109|      0|            return Ok(());
  110|      5|        }
  111|       |
  112|      9|        if entries.iter().any(|entry| entry.value == TOMBSTONE_VALUE) {
                         ^5             ^5
  113|      1|            return Err(GpuMapError::TombstoneValueReserved {
  114|      1|                value: TOMBSTONE_VALUE,
  115|      1|            });
  116|      4|        }
  117|       |
  118|      4|        let len = entries.len() as u32;
  119|      4|        let requested = self.slab.len() + len;
  120|      4|        if requested > self.slab.capacity() {
  121|      0|            return Err(GpuMapError::CapacityExceeded {
  122|      0|                capacity: self.slab.capacity(),
  123|      0|                requested,
  124|      0|            });
  125|      4|        }
  126|       |
  127|      4|        self.input.write(&self.queue, entries);
  128|      4|        let merge_len = self.bulk_put.execute(
  129|      4|            &self.slab,
  130|      4|            &self.input,
  131|      4|            &self.merge,
  132|      4|            &self.merge_meta,
  133|      4|            len,
  134|      0|        )?;
  135|      4|        self.update_len(merge_len);
  136|      4|        Ok(())
  137|      5|    }
  138|       |
  139|      2|    pub fn bulk_delete(&self, keys: &[u32]) {
  140|      2|        self.bulk_delete.execute(&self.slab, keys);
  141|      2|    }
  142|       |
  143|      3|    pub fn get(&self, key: u32) -> Option<u32> {
  144|      3|        self.bulk_get(&[key]).into_iter().next().unwrap_or(None)
  145|      3|    }
  146|       |
  147|      3|    pub fn put(&mut self, key: u32, value: u32) -> Result<(), GpuMapError> {
  148|      3|        let entry = KvEntry { key, value };
  149|      3|        self.bulk_put(std::slice::from_ref(&entry))
  150|      3|    }
  151|       |
  152|      1|    pub fn delete(&self, key: u32) {
  153|      1|        self.bulk_delete(std::slice::from_ref(&key));
  154|      1|    }
  155|       |
  156|      0|    pub fn capacity(&self) -> u32 {
  157|      0|        self.slab.capacity()
  158|      0|    }
  159|       |
  160|      0|    pub fn len(&self) -> u32 {
  161|      0|        self.slab.len()
  162|      0|    }
  163|       |
  164|      4|    pub fn update_len(&mut self, new_len: u32) {
  165|      4|        self.slab.update_len(&self.queue, new_len);
  166|      4|    }
  167|       |}
  168|       |
  169|       |#[derive(Debug, Clone, Copy, PartialEq, Eq)]
  170|       |pub enum GpuMapError {
  171|       |    CapacityExceeded { capacity: u32, requested: u32 },
  172|       |    TombstoneValueReserved { value: u32 },
  173|       |}
  174|       |
  175|       |#[cfg(test)]
  176|       |mod tests {
  177|       |    use super::{GpuSortedMap, KvEntry};
  178|       |
  179|       |    #[test]
  180|      1|    fn creates_gpu_sorted_map() {
  181|      1|        let map = pollster::block_on(GpuSortedMap::new(1024));
  182|      1|        assert!(map.is_ok(), "GpuSortedMap::new should succeed");
  183|      1|    }
  184|       |
  185|       |    #[test]
  186|      1|    fn put_then_get() {
  187|      1|        let mut map = pollster::block_on(GpuSortedMap::new(8)).unwrap();
  188|      1|        let entries = [
  189|      1|            KvEntry { key: 42, value: 7 },
  190|      1|            KvEntry { key: 7, value: 9 },
  191|      1|            KvEntry { key: 13, value: 1 },
  192|      1|        ];
  193|      1|        map.bulk_put(&entries).unwrap();
  194|       |
  195|      1|        let results = map.bulk_get(&[7, 13, 42, 99]);
  196|      1|        assert_eq!(results, vec![Some(9), Some(1), Some(7), None]);
  197|      1|    }
  198|       |
  199|       |    #[test]
  200|      1|    fn single_put_then_get() {
  201|      1|        let mut map = pollster::block_on(GpuSortedMap::new(4)).unwrap();
  202|      1|        map.put(5, 11).unwrap();
  203|      1|        assert_eq!(map.get(5), Some(11));
  204|      1|    }
  205|       |
  206|       |    #[test]
  207|      1|    fn single_get_missing_key() {
  208|      1|        let map = pollster::block_on(GpuSortedMap::new(4)).unwrap();
  209|      1|        assert_eq!(map.get(9), None);
  210|      1|    }
  211|       |
  212|       |    #[test]
  213|      1|    fn bulk_delete_clears_values() {
  214|      1|        let mut map = pollster::block_on(GpuSortedMap::new(8)).unwrap();
  215|      1|        let entries = [
  216|      1|            KvEntry { key: 1, value: 10 },
  217|      1|            KvEntry { key: 2, value: 20 },
  218|      1|            KvEntry { key: 3, value: 30 },
  219|      1|        ];
  220|      1|        map.bulk_put(&entries).unwrap();
  221|      1|        map.bulk_delete(&[1, 3]);
  222|      1|        let results = map.bulk_get(&[1, 2, 3]);
  223|      1|        assert_eq!(results, vec![None, Some(20), None]);
  224|      1|    }
  225|       |
  226|       |    #[test]
  227|      1|    fn delete_single_key() {
  228|      1|        let mut map = pollster::block_on(GpuSortedMap::new(4)).unwrap();
  229|      1|        map.put(9, 99).unwrap();
  230|      1|        map.delete(9);
  231|      1|        assert_eq!(map.get(9), None);
  232|      1|    }
  233|       |
  234|       |    #[test]
  235|      1|    fn put_rejects_tombstone_value() {
  236|      1|        let mut map = pollster::block_on(GpuSortedMap::new(4)).unwrap();
  237|      1|        let err = map.put(1, 0xFFFF_FFFF).unwrap_err();
  238|      1|        assert!(matches!(err, super::GpuMapError::TombstoneValueReserved { .. }));
                              ^0
  239|      1|    }
  240|       |}
/Users/kentb/Dropbox/Mac/Documents/GitHub/GPUSortedMap/src/pipeline.rs:
    1|       |use bytemuck::{Pod, Zeroable};
    2|       |use std::sync::Arc;
    3|       |
    4|       |use crate::gpu_array::{GpuArray, GpuStorage};
    5|       |use crate::KvEntry;
    6|       |
    7|       |pub const TOMBSTONE_VALUE: u32 = 0xFFFF_FFFF;
    8|       |
    9|       |const BULK_GET_BIND_SLAB: u32 = 0;
   10|       |const BULK_GET_BIND_SLAB_META: u32 = 1;
   11|       |const BULK_GET_BIND_KEYS: u32 = 2;
   12|       |const BULK_GET_BIND_KEYS_META: u32 = 3;
   13|       |const BULK_GET_BIND_RESULTS: u32 = 4;
   14|       |
   15|       |const BULK_DELETE_BIND_SLAB: u32 = 0;
   16|       |const BULK_DELETE_BIND_SLAB_META: u32 = 1;
   17|       |const BULK_DELETE_BIND_KEYS: u32 = 2;
   18|       |const BULK_DELETE_BIND_KEYS_META: u32 = 3;
   19|       |
   20|       |const BULK_SORT_BIND_INPUT: u32 = 0;
   21|       |const BULK_SORT_BIND_PARAMS: u32 = 1;
   22|       |
   23|       |const BULK_DEDUP_BIND_INPUT: u32 = 0;
   24|       |const BULK_DEDUP_BIND_PARAMS: u32 = 1;
   25|       |const BULK_DEDUP_BIND_META: u32 = 2;
   26|       |
   27|       |const BULK_MERGE_BIND_SLAB: u32 = 0;
   28|       |const BULK_MERGE_BIND_INPUT: u32 = 1;
   29|       |const BULK_MERGE_BIND_OUTPUT: u32 = 2;
   30|       |const BULK_MERGE_BIND_SLAB_META: u32 = 3;
   31|       |const BULK_MERGE_BIND_INPUT_META: u32 = 4;
   32|       |const BULK_MERGE_BIND_MERGE_META: u32 = 5;
   33|       |
   34|       |#[repr(C)]
   35|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   36|       |pub struct MergeMeta {
   37|       |    pub len: u32,
   38|       |    pub _pad: [u32; 3],
   39|       |}
   40|       |
   41|       |#[repr(C)]
   42|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   43|       |struct ResultEntry {
   44|       |    value: u32,
   45|       |    found: u32,
   46|       |    _pad: [u32; 2],
   47|       |}
   48|       |
   49|       |#[repr(C)]
   50|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   51|       |struct KeysMeta {
   52|       |    len: u32,
   53|       |    _pad: [u32; 3],
   54|       |}
   55|       |
   56|       |#[repr(C)]
   57|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   58|       |struct InputMeta {
   59|       |    len: u32,
   60|       |    _pad: [u32; 3],
   61|       |}
   62|       |
   63|       |#[repr(C)]
   64|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   65|       |struct SortParams {
   66|       |    k: u32,
   67|       |    j: u32,
   68|       |    len: u32,
   69|       |    _pad: u32,
   70|       |}
   71|       |
   72|       |#[repr(C)]
   73|       |#[derive(Clone, Copy, Pod, Zeroable, Debug, Default)]
   74|       |struct DedupParams {
   75|       |    len: u32,
   76|       |    _pad: [u32; 3],
   77|       |}
   78|       |
   79|       |pub struct BulkGetPipeline {
   80|       |    device: Arc<wgpu::Device>,
   81|       |    queue: Arc<wgpu::Queue>,
   82|       |    pipeline: wgpu::ComputePipeline,
   83|       |    bind_group_layout: wgpu::BindGroupLayout,
   84|       |}
   85|       |
   86|       |pub struct BulkDeletePipeline {
   87|       |    device: Arc<wgpu::Device>,
   88|       |    queue: Arc<wgpu::Queue>,
   89|       |    pipeline: wgpu::ComputePipeline,
   90|       |    bind_group_layout: wgpu::BindGroupLayout,
   91|       |}
   92|       |
   93|       |pub struct BulkPutPipeline {
   94|       |    device: Arc<wgpu::Device>,
   95|       |    queue: Arc<wgpu::Queue>,
   96|       |    sort_pipeline: wgpu::ComputePipeline,
   97|       |    sort_bind_group_layout: wgpu::BindGroupLayout,
   98|       |    dedup_pipeline: wgpu::ComputePipeline,
   99|       |    dedup_bind_group_layout: wgpu::BindGroupLayout,
  100|       |    merge_pipeline: wgpu::ComputePipeline,
  101|       |    merge_bind_group_layout: wgpu::BindGroupLayout,
  102|       |}
  103|       |
  104|       |impl BulkGetPipeline {
  105|      7|    pub fn new(device: Arc<wgpu::Device>, queue: Arc<wgpu::Queue>) -> Self {
  106|      7|        let bulk_get_shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
  107|      7|            label: Some("bulk-get-shader"),
  108|      7|            source: wgpu::ShaderSource::Wgsl(BULK_GET_WGSL.into()),
  109|      7|        });
  110|      7|        let bind_group_layout =
  111|      7|            device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
  112|      7|                label: Some("bulk-get-bind-group-layout"),
  113|      7|                entries: &[
  114|      7|                    wgpu::BindGroupLayoutEntry {
  115|      7|                        binding: BULK_GET_BIND_SLAB,
  116|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  117|      7|                        ty: wgpu::BindingType::Buffer {
  118|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: true },
  119|      7|                            has_dynamic_offset: false,
  120|      7|                            min_binding_size: None,
  121|      7|                        },
  122|      7|                        count: None,
  123|      7|                    },
  124|      7|                    wgpu::BindGroupLayoutEntry {
  125|      7|                        binding: BULK_GET_BIND_SLAB_META,
  126|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  127|      7|                        ty: wgpu::BindingType::Buffer {
  128|      7|                            ty: wgpu::BufferBindingType::Uniform,
  129|      7|                            has_dynamic_offset: false,
  130|      7|                            min_binding_size: None,
  131|      7|                        },
  132|      7|                        count: None,
  133|      7|                    },
  134|      7|                    wgpu::BindGroupLayoutEntry {
  135|      7|                        binding: BULK_GET_BIND_KEYS,
  136|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  137|      7|                        ty: wgpu::BindingType::Buffer {
  138|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: true },
  139|      7|                            has_dynamic_offset: false,
  140|      7|                            min_binding_size: None,
  141|      7|                        },
  142|      7|                        count: None,
  143|      7|                    },
  144|      7|                    wgpu::BindGroupLayoutEntry {
  145|      7|                        binding: BULK_GET_BIND_KEYS_META,
  146|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  147|      7|                        ty: wgpu::BindingType::Buffer {
  148|      7|                            ty: wgpu::BufferBindingType::Uniform,
  149|      7|                            has_dynamic_offset: false,
  150|      7|                            min_binding_size: None,
  151|      7|                        },
  152|      7|                        count: None,
  153|      7|                    },
  154|      7|                    wgpu::BindGroupLayoutEntry {
  155|      7|                        binding: BULK_GET_BIND_RESULTS,
  156|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  157|      7|                        ty: wgpu::BindingType::Buffer {
  158|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  159|      7|                            has_dynamic_offset: false,
  160|      7|                            min_binding_size: None,
  161|      7|                        },
  162|      7|                        count: None,
  163|      7|                    },
  164|      7|                ],
  165|      7|            });
  166|      7|        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
  167|      7|            label: Some("bulk-get-pipeline-layout"),
  168|      7|            bind_group_layouts: &[&bind_group_layout],
  169|      7|            push_constant_ranges: &[],
  170|      7|        });
  171|      7|        let pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
  172|      7|            label: Some("bulk-get-pipeline"),
  173|      7|            layout: Some(&pipeline_layout),
  174|      7|            module: &bulk_get_shader,
  175|      7|            entry_point: "main",
  176|      7|            compilation_options: wgpu::PipelineCompilationOptions::default(),
  177|      7|        });
  178|       |
  179|      7|        Self {
  180|      7|            device,
  181|      7|            queue,
  182|      7|            pipeline,
  183|      7|            bind_group_layout,
  184|      7|        }
  185|      7|    }
  186|       |
  187|      5|    fn bind_group(
  188|      5|        &self,
  189|      5|        slab: &wgpu::Buffer,
  190|      5|        slab_meta: &wgpu::Buffer,
  191|      5|        keys: &wgpu::Buffer,
  192|      5|        keys_meta: &wgpu::Buffer,
  193|      5|        results: &wgpu::Buffer,
  194|      5|    ) -> wgpu::BindGroup {
  195|      5|        self.device.create_bind_group(&wgpu::BindGroupDescriptor {
  196|      5|            label: Some("bulk-get-bind-group"),
  197|      5|            layout: &self.bind_group_layout,
  198|      5|            entries: &[
  199|      5|                wgpu::BindGroupEntry {
  200|      5|                    binding: BULK_GET_BIND_SLAB,
  201|      5|                    resource: slab.as_entire_binding(),
  202|      5|                },
  203|      5|                wgpu::BindGroupEntry {
  204|      5|                    binding: BULK_GET_BIND_SLAB_META,
  205|      5|                    resource: slab_meta.as_entire_binding(),
  206|      5|                },
  207|      5|                wgpu::BindGroupEntry {
  208|      5|                    binding: BULK_GET_BIND_KEYS,
  209|      5|                    resource: keys.as_entire_binding(),
  210|      5|                },
  211|      5|                wgpu::BindGroupEntry {
  212|      5|                    binding: BULK_GET_BIND_KEYS_META,
  213|      5|                    resource: keys_meta.as_entire_binding(),
  214|      5|                },
  215|      5|                wgpu::BindGroupEntry {
  216|      5|                    binding: BULK_GET_BIND_RESULTS,
  217|      5|                    resource: results.as_entire_binding(),
  218|      5|                },
  219|      5|            ],
  220|      5|        })
  221|      5|    }
  222|       |
  223|      5|    pub fn execute(&self, slab: &GpuArray<KvEntry>, keys: &[u32]) -> Vec<Option<u32>> {
  224|      5|        if keys.is_empty() {
  225|      0|            return Vec::new();
  226|      5|        }
  227|       |
  228|      5|        let keys_buffer = create_buffer_with_data(
  229|      5|            &self.device,
  230|      5|            "keys-buffer",
  231|      5|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
  232|      5|            keys,
  233|       |        );
  234|      5|        let results_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
  235|      5|            label: Some("results-buffer"),
  236|      5|            size: (keys.len() * std::mem::size_of::<ResultEntry>()) as u64,
  237|      5|            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC,
  238|      5|            mapped_at_creation: false,
  239|      5|        });
  240|      5|        let readback_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
  241|      5|            label: Some("results-readback-buffer"),
  242|      5|            size: (keys.len() * std::mem::size_of::<ResultEntry>()) as u64,
  243|      5|            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
  244|      5|            mapped_at_creation: false,
  245|      5|        });
  246|      5|        let keys_meta = KeysMeta {
  247|      5|            len: keys.len() as u32,
  248|      5|            _pad: [0; 3],
  249|      5|        };
  250|      5|        let keys_meta_buffer = create_buffer_with_data(
  251|      5|            &self.device,
  252|      5|            "keys-meta-buffer",
  253|      5|            wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
  254|      5|            &[keys_meta],
  255|       |        );
  256|       |
  257|      5|        let bind_group = self.bind_group(
  258|      5|            slab.buffer(),
  259|      5|            slab.meta_buffer(),
  260|      5|            &keys_buffer,
  261|      5|            &keys_meta_buffer,
  262|      5|            &results_buffer,
  263|       |        );
  264|       |
  265|      5|        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  266|      5|            label: Some("bulk-get-encoder"),
  267|      5|        });
  268|      5|        {
  269|      5|            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
  270|      5|                label: Some("bulk-get-pass"),
  271|      5|                timestamp_writes: None,
  272|      5|            });
  273|      5|            cpass.set_pipeline(&self.pipeline);
  274|      5|            cpass.set_bind_group(0, &bind_group, &[]);
  275|      5|            let workgroups = ((keys.len() as u32) + 63) / 64;
  276|      5|            cpass.dispatch_workgroups(workgroups, 1, 1);
  277|      5|        }
  278|      5|        encoder.copy_buffer_to_buffer(
  279|      5|            &results_buffer,
  280|       |            0,
  281|      5|            &readback_buffer,
  282|       |            0,
  283|      5|            (keys.len() * std::mem::size_of::<ResultEntry>()) as u64,
  284|       |        );
  285|      5|        self.queue.submit(Some(encoder.finish()));
  286|       |
  287|      5|        let result_entries = readback_vec::<ResultEntry>(&self.device, &readback_buffer);
  288|      5|        result_entries
  289|      5|            .iter()
  290|     10|            .map(|entry| {
                           ^5
  291|     10|                if entry.found == 0 || entry.value == TOMBSTONE_VALUE {
                                                     ^8
  292|      5|                    None
  293|       |                } else {
  294|      5|                    Some(entry.value)
  295|       |                }
  296|     10|            })
  297|      5|            .collect()
  298|      5|    }
  299|       |}
  300|       |
  301|       |impl BulkDeletePipeline {
  302|      7|    pub fn new(device: Arc<wgpu::Device>, queue: Arc<wgpu::Queue>) -> Self {
  303|      7|        let bulk_delete_shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
  304|      7|            label: Some("bulk-delete-shader"),
  305|      7|            source: wgpu::ShaderSource::Wgsl(BULK_DELETE_WGSL.into()),
  306|      7|        });
  307|      7|        let bind_group_layout =
  308|      7|            device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
  309|      7|                label: Some("bulk-delete-bind-group-layout"),
  310|      7|                entries: &[
  311|      7|                    wgpu::BindGroupLayoutEntry {
  312|      7|                        binding: BULK_DELETE_BIND_SLAB,
  313|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  314|      7|                        ty: wgpu::BindingType::Buffer {
  315|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  316|      7|                            has_dynamic_offset: false,
  317|      7|                            min_binding_size: None,
  318|      7|                        },
  319|      7|                        count: None,
  320|      7|                    },
  321|      7|                    wgpu::BindGroupLayoutEntry {
  322|      7|                        binding: BULK_DELETE_BIND_SLAB_META,
  323|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  324|      7|                        ty: wgpu::BindingType::Buffer {
  325|      7|                            ty: wgpu::BufferBindingType::Uniform,
  326|      7|                            has_dynamic_offset: false,
  327|      7|                            min_binding_size: None,
  328|      7|                        },
  329|      7|                        count: None,
  330|      7|                    },
  331|      7|                    wgpu::BindGroupLayoutEntry {
  332|      7|                        binding: BULK_DELETE_BIND_KEYS,
  333|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  334|      7|                        ty: wgpu::BindingType::Buffer {
  335|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: true },
  336|      7|                            has_dynamic_offset: false,
  337|      7|                            min_binding_size: None,
  338|      7|                        },
  339|      7|                        count: None,
  340|      7|                    },
  341|      7|                    wgpu::BindGroupLayoutEntry {
  342|      7|                        binding: BULK_DELETE_BIND_KEYS_META,
  343|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  344|      7|                        ty: wgpu::BindingType::Buffer {
  345|      7|                            ty: wgpu::BufferBindingType::Uniform,
  346|      7|                            has_dynamic_offset: false,
  347|      7|                            min_binding_size: None,
  348|      7|                        },
  349|      7|                        count: None,
  350|      7|                    },
  351|      7|                ],
  352|      7|            });
  353|      7|        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
  354|      7|            label: Some("bulk-delete-pipeline-layout"),
  355|      7|            bind_group_layouts: &[&bind_group_layout],
  356|      7|            push_constant_ranges: &[],
  357|      7|        });
  358|      7|        let pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
  359|      7|            label: Some("bulk-delete-pipeline"),
  360|      7|            layout: Some(&pipeline_layout),
  361|      7|            module: &bulk_delete_shader,
  362|      7|            entry_point: "main",
  363|      7|            compilation_options: wgpu::PipelineCompilationOptions::default(),
  364|      7|        });
  365|       |
  366|      7|        Self {
  367|      7|            device,
  368|      7|            queue,
  369|      7|            pipeline,
  370|      7|            bind_group_layout,
  371|      7|        }
  372|      7|    }
  373|       |
  374|      2|    pub fn execute(&self, slab: &GpuArray<KvEntry>, keys: &[u32]) {
  375|      2|        if keys.is_empty() {
  376|      0|            return;
  377|      2|        }
  378|       |
  379|      2|        let keys_buffer = create_buffer_with_data(
  380|      2|            &self.device,
  381|      2|            "delete-keys-buffer",
  382|      2|            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
  383|      2|            keys,
  384|       |        );
  385|      2|        let keys_meta = KeysMeta {
  386|      2|            len: keys.len() as u32,
  387|      2|            _pad: [0; 3],
  388|      2|        };
  389|      2|        let keys_meta_buffer = create_buffer_with_data(
  390|      2|            &self.device,
  391|      2|            "delete-keys-meta-buffer",
  392|      2|            wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
  393|      2|            &[keys_meta],
  394|       |        );
  395|       |
  396|      2|        let bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
  397|      2|            label: Some("bulk-delete-bind-group"),
  398|      2|            layout: &self.bind_group_layout,
  399|      2|            entries: &[
  400|      2|                wgpu::BindGroupEntry {
  401|      2|                    binding: BULK_DELETE_BIND_SLAB,
  402|      2|                    resource: slab.buffer().as_entire_binding(),
  403|      2|                },
  404|      2|                wgpu::BindGroupEntry {
  405|      2|                    binding: BULK_DELETE_BIND_SLAB_META,
  406|      2|                    resource: slab.meta_buffer().as_entire_binding(),
  407|      2|                },
  408|      2|                wgpu::BindGroupEntry {
  409|      2|                    binding: BULK_DELETE_BIND_KEYS,
  410|      2|                    resource: keys_buffer.as_entire_binding(),
  411|      2|                },
  412|      2|                wgpu::BindGroupEntry {
  413|      2|                    binding: BULK_DELETE_BIND_KEYS_META,
  414|      2|                    resource: keys_meta_buffer.as_entire_binding(),
  415|      2|                },
  416|      2|            ],
  417|      2|        });
  418|       |
  419|      2|        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  420|      2|            label: Some("bulk-delete-encoder"),
  421|      2|        });
  422|      2|        {
  423|      2|            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
  424|      2|                label: Some("bulk-delete-pass"),
  425|      2|                timestamp_writes: None,
  426|      2|            });
  427|      2|            cpass.set_pipeline(&self.pipeline);
  428|      2|            cpass.set_bind_group(0, &bind_group, &[]);
  429|      2|            let workgroups = ((keys.len() as u32) + 63) / 64;
  430|      2|            cpass.dispatch_workgroups(workgroups, 1, 1);
  431|      2|        }
  432|      2|        self.queue.submit(Some(encoder.finish()));
  433|      2|    }
  434|       |}
  435|       |
  436|       |impl BulkPutPipeline {
  437|      7|    pub fn new(device: Arc<wgpu::Device>, queue: Arc<wgpu::Queue>) -> Self {
  438|      7|        let sort_shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
  439|      7|            label: Some("bulk-sort-shader"),
  440|      7|            source: wgpu::ShaderSource::Wgsl(BULK_SORT_WGSL.into()),
  441|      7|        });
  442|      7|        let sort_bind_group_layout =
  443|      7|            device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
  444|      7|                label: Some("bulk-sort-bind-group-layout"),
  445|      7|                entries: &[
  446|      7|                    wgpu::BindGroupLayoutEntry {
  447|      7|                        binding: BULK_SORT_BIND_INPUT,
  448|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  449|      7|                        ty: wgpu::BindingType::Buffer {
  450|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  451|      7|                            has_dynamic_offset: false,
  452|      7|                            min_binding_size: None,
  453|      7|                        },
  454|      7|                        count: None,
  455|      7|                    },
  456|      7|                    wgpu::BindGroupLayoutEntry {
  457|      7|                        binding: BULK_SORT_BIND_PARAMS,
  458|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  459|      7|                        ty: wgpu::BindingType::Buffer {
  460|      7|                            ty: wgpu::BufferBindingType::Uniform,
  461|      7|                            has_dynamic_offset: false,
  462|      7|                            min_binding_size: None,
  463|      7|                        },
  464|      7|                        count: None,
  465|      7|                    },
  466|      7|                ],
  467|      7|            });
  468|      7|        let sort_pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
  469|      7|            label: Some("bulk-sort-pipeline-layout"),
  470|      7|            bind_group_layouts: &[&sort_bind_group_layout],
  471|      7|            push_constant_ranges: &[],
  472|      7|        });
  473|      7|        let sort_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
  474|      7|            label: Some("bulk-sort-pipeline"),
  475|      7|            layout: Some(&sort_pipeline_layout),
  476|      7|            module: &sort_shader,
  477|      7|            entry_point: "main",
  478|      7|            compilation_options: wgpu::PipelineCompilationOptions::default(),
  479|      7|        });
  480|       |
  481|      7|        let dedup_shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
  482|      7|            label: Some("bulk-dedup-shader"),
  483|      7|            source: wgpu::ShaderSource::Wgsl(BULK_DEDUP_WGSL.into()),
  484|      7|        });
  485|      7|        let dedup_bind_group_layout =
  486|      7|            device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
  487|      7|                label: Some("bulk-dedup-bind-group-layout"),
  488|      7|                entries: &[
  489|      7|                    wgpu::BindGroupLayoutEntry {
  490|      7|                        binding: BULK_DEDUP_BIND_INPUT,
  491|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  492|      7|                        ty: wgpu::BindingType::Buffer {
  493|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  494|      7|                            has_dynamic_offset: false,
  495|      7|                            min_binding_size: None,
  496|      7|                        },
  497|      7|                        count: None,
  498|      7|                    },
  499|      7|                    wgpu::BindGroupLayoutEntry {
  500|      7|                        binding: BULK_DEDUP_BIND_PARAMS,
  501|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  502|      7|                        ty: wgpu::BindingType::Buffer {
  503|      7|                            ty: wgpu::BufferBindingType::Uniform,
  504|      7|                            has_dynamic_offset: false,
  505|      7|                            min_binding_size: None,
  506|      7|                        },
  507|      7|                        count: None,
  508|      7|                    },
  509|      7|                    wgpu::BindGroupLayoutEntry {
  510|      7|                        binding: BULK_DEDUP_BIND_META,
  511|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  512|      7|                        ty: wgpu::BindingType::Buffer {
  513|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  514|      7|                            has_dynamic_offset: false,
  515|      7|                            min_binding_size: None,
  516|      7|                        },
  517|      7|                        count: None,
  518|      7|                    },
  519|      7|                ],
  520|      7|            });
  521|      7|        let dedup_pipeline_layout =
  522|      7|            device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
  523|      7|                label: Some("bulk-dedup-pipeline-layout"),
  524|      7|                bind_group_layouts: &[&dedup_bind_group_layout],
  525|      7|                push_constant_ranges: &[],
  526|      7|            });
  527|      7|        let dedup_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
  528|      7|            label: Some("bulk-dedup-pipeline"),
  529|      7|            layout: Some(&dedup_pipeline_layout),
  530|      7|            module: &dedup_shader,
  531|      7|            entry_point: "main",
  532|      7|            compilation_options: wgpu::PipelineCompilationOptions::default(),
  533|      7|        });
  534|       |
  535|      7|        let merge_shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
  536|      7|            label: Some("bulk-merge-shader"),
  537|      7|            source: wgpu::ShaderSource::Wgsl(BULK_MERGE_WGSL.into()),
  538|      7|        });
  539|      7|        let merge_bind_group_layout =
  540|      7|            device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
  541|      7|                label: Some("bulk-merge-bind-group-layout"),
  542|      7|                entries: &[
  543|      7|                    wgpu::BindGroupLayoutEntry {
  544|      7|                        binding: BULK_MERGE_BIND_SLAB,
  545|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  546|      7|                        ty: wgpu::BindingType::Buffer {
  547|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: true },
  548|      7|                            has_dynamic_offset: false,
  549|      7|                            min_binding_size: None,
  550|      7|                        },
  551|      7|                        count: None,
  552|      7|                    },
  553|      7|                    wgpu::BindGroupLayoutEntry {
  554|      7|                        binding: BULK_MERGE_BIND_INPUT,
  555|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  556|      7|                        ty: wgpu::BindingType::Buffer {
  557|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: true },
  558|      7|                            has_dynamic_offset: false,
  559|      7|                            min_binding_size: None,
  560|      7|                        },
  561|      7|                        count: None,
  562|      7|                    },
  563|      7|                    wgpu::BindGroupLayoutEntry {
  564|      7|                        binding: BULK_MERGE_BIND_OUTPUT,
  565|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  566|      7|                        ty: wgpu::BindingType::Buffer {
  567|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  568|      7|                            has_dynamic_offset: false,
  569|      7|                            min_binding_size: None,
  570|      7|                        },
  571|      7|                        count: None,
  572|      7|                    },
  573|      7|                    wgpu::BindGroupLayoutEntry {
  574|      7|                        binding: BULK_MERGE_BIND_SLAB_META,
  575|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  576|      7|                        ty: wgpu::BindingType::Buffer {
  577|      7|                            ty: wgpu::BufferBindingType::Uniform,
  578|      7|                            has_dynamic_offset: false,
  579|      7|                            min_binding_size: None,
  580|      7|                        },
  581|      7|                        count: None,
  582|      7|                    },
  583|      7|                    wgpu::BindGroupLayoutEntry {
  584|      7|                        binding: BULK_MERGE_BIND_INPUT_META,
  585|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  586|      7|                        ty: wgpu::BindingType::Buffer {
  587|      7|                            ty: wgpu::BufferBindingType::Uniform,
  588|      7|                            has_dynamic_offset: false,
  589|      7|                            min_binding_size: None,
  590|      7|                        },
  591|      7|                        count: None,
  592|      7|                    },
  593|      7|                    wgpu::BindGroupLayoutEntry {
  594|      7|                        binding: BULK_MERGE_BIND_MERGE_META,
  595|      7|                        visibility: wgpu::ShaderStages::COMPUTE,
  596|      7|                        ty: wgpu::BindingType::Buffer {
  597|      7|                            ty: wgpu::BufferBindingType::Storage { read_only: false },
  598|      7|                            has_dynamic_offset: false,
  599|      7|                            min_binding_size: None,
  600|      7|                        },
  601|      7|                        count: None,
  602|      7|                    },
  603|      7|                ],
  604|      7|            });
  605|      7|        let merge_pipeline_layout =
  606|      7|            device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
  607|      7|                label: Some("bulk-merge-pipeline-layout"),
  608|      7|                bind_group_layouts: &[&merge_bind_group_layout],
  609|      7|                push_constant_ranges: &[],
  610|      7|            });
  611|      7|        let merge_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
  612|      7|            label: Some("bulk-merge-pipeline"),
  613|      7|            layout: Some(&merge_pipeline_layout),
  614|      7|            module: &merge_shader,
  615|      7|            entry_point: "main",
  616|      7|            compilation_options: wgpu::PipelineCompilationOptions::default(),
  617|      7|        });
  618|       |
  619|      7|        Self {
  620|      7|            device,
  621|      7|            queue,
  622|      7|            sort_pipeline,
  623|      7|            sort_bind_group_layout,
  624|      7|            dedup_pipeline,
  625|      7|            dedup_bind_group_layout,
  626|      7|            merge_pipeline,
  627|      7|            merge_bind_group_layout,
  628|      7|        }
  629|      7|    }
  630|       |
  631|      4|    pub fn execute(
  632|      4|        &self,
  633|      4|        slab: &GpuArray<KvEntry>,
  634|      4|        input: &GpuArray<KvEntry>,
  635|      4|        merge: &GpuArray<KvEntry>,
  636|      4|        merge_meta: &GpuStorage<MergeMeta>,
  637|      4|        entries_len: u32,
  638|      4|    ) -> Result<u32, crate::GpuMapError> {
  639|      4|        let len = entries_len;
  640|      4|        let padded_len = len.next_power_of_two();
  641|      4|        if padded_len > slab.capacity() {
  642|      0|            return Err(crate::GpuMapError::CapacityExceeded {
  643|      0|                capacity: slab.capacity(),
  644|      0|                requested: slab.len() + padded_len,
  645|      0|            });
  646|      4|        }
  647|       |
  648|      4|        if padded_len > len {
  649|      2|            let pad_count = (padded_len - len) as usize;
  650|      2|            let padding = vec![
  651|      2|                KvEntry {
  652|      2|                    key: u32::MAX,
  653|      2|                    value: 0,
  654|      2|                };
  655|      2|                pad_count
  656|      2|            ];
  657|      2|            let offset = (len as u64) * std::mem::size_of::<KvEntry>() as u64;
  658|      2|            self.queue
  659|      2|                .write_buffer(input.buffer(), offset, bytemuck::cast_slice(&padding));
  660|      2|        }
  661|       |
  662|      4|        if len > 1 {
  663|      2|            let sort_params_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
  664|      2|                label: Some("bulk-sort-params"),
  665|      2|                size: std::mem::size_of::<SortParams>() as u64,
  666|      2|                usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
  667|      2|                mapped_at_creation: false,
  668|      2|            });
  669|      2|            let sort_bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
  670|      2|                label: Some("bulk-sort-bind-group"),
  671|      2|                layout: &self.sort_bind_group_layout,
  672|      2|                entries: &[
  673|      2|                    wgpu::BindGroupEntry {
  674|      2|                        binding: BULK_SORT_BIND_INPUT,
  675|      2|                        resource: input.buffer().as_entire_binding(),
  676|      2|                    },
  677|      2|                    wgpu::BindGroupEntry {
  678|      2|                        binding: BULK_SORT_BIND_PARAMS,
  679|      2|                        resource: sort_params_buffer.as_entire_binding(),
  680|      2|                    },
  681|      2|                ],
  682|      2|            });
  683|       |
  684|      2|            let workgroups = ((padded_len + 63) / 64) as u32;
  685|      2|            let mut k = 2u32;
  686|      2|            let max_k = padded_len;
  687|      6|            while k <= max_k {
  688|      4|                let mut j = k / 2;
  689|     10|                while j > 0 {
  690|      6|                    let params = SortParams {
  691|      6|                        k,
  692|      6|                        j,
  693|      6|                        len: padded_len,
  694|      6|                        _pad: 0,
  695|      6|                    };
  696|      6|                    self.queue
  697|      6|                        .write_buffer(&sort_params_buffer, 0, bytemuck::bytes_of(&params));
  698|      6|
  699|      6|                    let mut sort_encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  700|      6|                        label: Some("bulk-sort-encoder"),
  701|      6|                    });
  702|      6|                    {
  703|      6|                        let mut cpass = sort_encoder.begin_compute_pass(
  704|      6|                            &wgpu::ComputePassDescriptor {
  705|      6|                                label: Some("bulk-sort-pass"),
  706|      6|                                timestamp_writes: None,
  707|      6|                            },
  708|      6|                        );
  709|      6|                        cpass.set_pipeline(&self.sort_pipeline);
  710|      6|                        cpass.set_bind_group(0, &sort_bind_group, &[]);
  711|      6|                        cpass.dispatch_workgroups(workgroups, 1, 1);
  712|      6|                    }
  713|      6|                    self.queue.submit(Some(sort_encoder.finish()));
  714|      6|                    j /= 2;
  715|      6|                }
  716|      4|                k *= 2;
  717|       |            }
  718|      2|        }
  719|       |
  720|      4|        let dedup_params = DedupParams { len, _pad: [0; 3] };
  721|      4|        let dedup_params_buffer = create_buffer_with_data(
  722|      4|            &self.device,
  723|      4|            "bulk-dedup-params",
  724|      4|            wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
  725|      4|            &[dedup_params],
  726|       |        );
  727|      4|        let dedup_bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
  728|      4|            label: Some("bulk-dedup-bind-group"),
  729|      4|            layout: &self.dedup_bind_group_layout,
  730|      4|            entries: &[
  731|      4|                wgpu::BindGroupEntry {
  732|      4|                    binding: BULK_DEDUP_BIND_INPUT,
  733|      4|                    resource: input.buffer().as_entire_binding(),
  734|      4|                },
  735|      4|                wgpu::BindGroupEntry {
  736|      4|                    binding: BULK_DEDUP_BIND_PARAMS,
  737|      4|                    resource: dedup_params_buffer.as_entire_binding(),
  738|      4|                },
  739|      4|                wgpu::BindGroupEntry {
  740|      4|                    binding: BULK_DEDUP_BIND_META,
  741|      4|                    resource: merge_meta.buffer().as_entire_binding(),
  742|      4|                },
  743|      4|            ],
  744|      4|        });
  745|       |
  746|      4|        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  747|      4|            label: Some("bulk-put-encoder"),
  748|      4|        });
  749|      4|        {
  750|      4|            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
  751|      4|                label: Some("bulk-dedup-pass"),
  752|      4|                timestamp_writes: None,
  753|      4|            });
  754|      4|            cpass.set_pipeline(&self.dedup_pipeline);
  755|      4|            cpass.set_bind_group(0, &dedup_bind_group, &[]);
  756|      4|            cpass.dispatch_workgroups(1, 1, 1);
  757|      4|        }
  758|       |
  759|      4|        let dedup_readback = self.device.create_buffer(&wgpu::BufferDescriptor {
  760|      4|            label: Some("bulk-dedup-readback"),
  761|      4|            size: std::mem::size_of::<MergeMeta>() as u64,
  762|      4|            usage: wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::MAP_READ,
  763|      4|            mapped_at_creation: false,
  764|      4|        });
  765|       |
  766|      4|        encoder.copy_buffer_to_buffer(
  767|      4|            merge_meta.buffer(),
  768|       |            0,
  769|      4|            &dedup_readback,
  770|       |            0,
  771|      4|            std::mem::size_of::<MergeMeta>() as u64,
  772|       |        );
  773|       |
  774|      4|        self.queue.submit(Some(encoder.finish()));
  775|      4|        let dedup_meta = readback_single::<MergeMeta>(&self.device, &dedup_readback);
  776|      4|        let dedup_len = dedup_meta.len;
  777|       |
  778|      4|        let input_meta = InputMeta {
  779|      4|            len: dedup_len,
  780|      4|            _pad: [0; 3],
  781|      4|        };
  782|      4|        let input_meta_buffer = create_buffer_with_data(
  783|      4|            &self.device,
  784|      4|            "input-meta-buffer",
  785|      4|            wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
  786|      4|            &[input_meta],
  787|       |        );
  788|       |
  789|      4|        let merge_bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
  790|      4|            label: Some("bulk-merge-bind-group"),
  791|      4|            layout: &self.merge_bind_group_layout,
  792|      4|            entries: &[
  793|      4|                wgpu::BindGroupEntry {
  794|      4|                    binding: BULK_MERGE_BIND_SLAB,
  795|      4|                    resource: slab.buffer().as_entire_binding(),
  796|      4|                },
  797|      4|                wgpu::BindGroupEntry {
  798|      4|                    binding: BULK_MERGE_BIND_INPUT,
  799|      4|                    resource: input.buffer().as_entire_binding(),
  800|      4|                },
  801|      4|                wgpu::BindGroupEntry {
  802|      4|                    binding: BULK_MERGE_BIND_OUTPUT,
  803|      4|                    resource: merge.buffer().as_entire_binding(),
  804|      4|                },
  805|      4|                wgpu::BindGroupEntry {
  806|      4|                    binding: BULK_MERGE_BIND_SLAB_META,
  807|      4|                    resource: slab.meta_buffer().as_entire_binding(),
  808|      4|                },
  809|      4|                wgpu::BindGroupEntry {
  810|      4|                    binding: BULK_MERGE_BIND_INPUT_META,
  811|      4|                    resource: input_meta_buffer.as_entire_binding(),
  812|      4|                },
  813|      4|                wgpu::BindGroupEntry {
  814|      4|                    binding: BULK_MERGE_BIND_MERGE_META,
  815|      4|                    resource: merge_meta.buffer().as_entire_binding(),
  816|      4|                },
  817|      4|            ],
  818|      4|        });
  819|       |
  820|      4|        let merge_readback = self.device.create_buffer(&wgpu::BufferDescriptor {
  821|      4|            label: Some("merge-meta-readback"),
  822|      4|            size: std::mem::size_of::<MergeMeta>() as u64,
  823|      4|            usage: wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::MAP_READ,
  824|      4|            mapped_at_creation: false,
  825|      4|        });
  826|       |
  827|      4|        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
  828|      4|            label: Some("bulk-merge-encoder"),
  829|      4|        });
  830|      4|        {
  831|      4|            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
  832|      4|                label: Some("bulk-merge-pass"),
  833|      4|                timestamp_writes: None,
  834|      4|            });
  835|      4|            cpass.set_pipeline(&self.merge_pipeline);
  836|      4|            cpass.set_bind_group(0, &merge_bind_group, &[]);
  837|      4|            cpass.dispatch_workgroups(1, 1, 1);
  838|      4|        }
  839|       |
  840|      4|        let slab_bytes = (slab.capacity() as u64) * std::mem::size_of::<KvEntry>() as u64;
  841|      4|        encoder.copy_buffer_to_buffer(merge.buffer(), 0, slab.buffer(), 0, slab_bytes);
  842|      4|        encoder.copy_buffer_to_buffer(
  843|      4|            merge_meta.buffer(),
  844|       |            0,
  845|      4|            &merge_readback,
  846|       |            0,
  847|      4|            std::mem::size_of::<MergeMeta>() as u64,
  848|       |        );
  849|      4|        self.queue.submit(Some(encoder.finish()));
  850|       |
  851|      4|        let merge_meta_val = readback_single::<MergeMeta>(&self.device, &merge_readback);
  852|      4|        Ok(merge_meta_val.len)
  853|      4|    }
  854|       |}
  855|       |
  856|     46|pub fn create_buffer_with_data<T: Pod>(
  857|     46|    device: &wgpu::Device,
  858|     46|    label: &str,
  859|     46|    usage: wgpu::BufferUsages,
  860|     46|    data: &[T],
  861|     46|) -> wgpu::Buffer {
  862|     46|    let size = (data.len() * std::mem::size_of::<T>()) as u64;
  863|     46|    let buffer = device.create_buffer(&wgpu::BufferDescriptor {
  864|     46|        label: Some(label),
  865|     46|        size,
  866|     46|        usage,
  867|     46|        mapped_at_creation: true,
  868|     46|    });
  869|     46|    if !data.is_empty() {
  870|     46|        let mut view = buffer.slice(..).get_mapped_range_mut();
  871|     46|        view.copy_from_slice(bytemuck::cast_slice(data));
  872|     46|    }
                  ^0
  873|     46|    buffer.unmap();
  874|     46|    buffer
  875|     46|}
  876|       |
  877|     15|pub fn readback_vec<T: Pod>(device: &wgpu::Device, buffer: &wgpu::Buffer) -> Vec<T> {
  878|     15|    let slice = buffer.slice(..);
  879|     15|    let (sender, receiver) = std::sync::mpsc::channel();
  880|     15|    slice.map_async(wgpu::MapMode::Read, move |res| {
  881|     15|        let _ = sender.send(res);
  882|     15|    });
  883|     15|    device.poll(wgpu::Maintain::Wait);
  884|     15|    receiver.recv().expect("readback channel closed").unwrap();
  885|     15|    let data = slice.get_mapped_range();
  886|     15|    let results = bytemuck::cast_slice(&data).to_vec();
  887|     15|    drop(data);
  888|     15|    buffer.unmap();
  889|     15|    results
  890|     15|}
  891|       |
  892|      8|pub(crate) fn readback_single<T: Pod>(device: &wgpu::Device, buffer: &wgpu::Buffer) -> T {
  893|      8|    readback_vec::<T>(device, buffer)
  894|      8|        .first()
  895|      8|        .copied()
  896|      8|        .expect("readback failed to return data")
  897|      8|}
  898|       |
  899|       |const BULK_GET_WGSL: &str = r#"
  900|       |struct KvEntry {
  901|       |    key: u32,
  902|       |    value: u32,
  903|       |};
  904|       |
  905|       |struct SlabMeta {
  906|       |    len: u32,
  907|       |    capacity: u32,
  908|       |    _pad0: u32,
  909|       |    _pad1: u32,
  910|       |};
  911|       |
  912|       |struct KeysMeta {
  913|       |    len: u32,
  914|       |    _pad0: u32,
  915|       |    _pad1: u32,
  916|       |    _pad2: u32,
  917|       |};
  918|       |
  919|       |struct ResultEntry {
  920|       |    value: u32,
  921|       |    found: u32,
  922|       |    _pad0: u32,
  923|       |    _pad1: u32,
  924|       |};
  925|       |
  926|       |@group(0) @binding(0) var<storage, read> slab: array<KvEntry>;
  927|       |@group(0) @binding(1) var<uniform> slab_meta: SlabMeta;
  928|       |@group(0) @binding(2) var<storage, read> keys: array<u32>;
  929|       |@group(0) @binding(3) var<uniform> keys_meta: KeysMeta;
  930|       |@group(0) @binding(4) var<storage, read_write> results: array<ResultEntry>;
  931|       |
  932|       |@compute @workgroup_size(64)
  933|       |fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  934|       |    let idx = gid.x;
  935|       |    if (idx >= keys_meta.len) {
  936|       |        return;
  937|       |    }
  938|       |
  939|       |    let key = keys[idx];
  940|       |    var lo: u32 = 0u;
  941|       |    var hi: u32 = slab_meta.len;
  942|       |    while (lo < hi) {
  943|       |        let mid = (lo + hi) / 2u;
  944|       |        let mid_key = slab[mid].key;
  945|       |        if (mid_key < key) {
  946|       |            lo = mid + 1u;
  947|       |        } else {
  948|       |            hi = mid;
  949|       |        }
  950|       |    }
  951|       |
  952|       |    if (lo < slab_meta.len && slab[lo].key == key) {
  953|       |        results[idx].value = slab[lo].value;
  954|       |        results[idx].found = 1u;
  955|       |    } else {
  956|       |        results[idx].value = 0u;
  957|       |        results[idx].found = 0u;
  958|       |    }
  959|       |}
  960|       |"#;
  961|       |
  962|       |const BULK_DELETE_WGSL: &str = r#"
  963|       |struct KvEntry {
  964|       |    key: u32,
  965|       |    value: u32,
  966|       |};
  967|       |
  968|       |struct SlabMeta {
  969|       |    len: u32,
  970|       |    capacity: u32,
  971|       |    _pad0: u32,
  972|       |    _pad1: u32,
  973|       |};
  974|       |
  975|       |struct KeysMeta {
  976|       |    len: u32,
  977|       |    _pad0: u32,
  978|       |    _pad1: u32,
  979|       |    _pad2: u32,
  980|       |};
  981|       |
  982|       |@group(0) @binding(0) var<storage, read_write> slab: array<KvEntry>;
  983|       |@group(0) @binding(1) var<uniform> slab_meta: SlabMeta;
  984|       |@group(0) @binding(2) var<storage, read> keys: array<u32>;
  985|       |@group(0) @binding(3) var<uniform> keys_meta: KeysMeta;
  986|       |
  987|       |@compute @workgroup_size(64)
  988|       |fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  989|       |    let idx = gid.x;
  990|       |    if (idx >= keys_meta.len) {
  991|       |        return;
  992|       |    }
  993|       |
  994|       |    let key = keys[idx];
  995|       |    var lo: u32 = 0u;
  996|       |    var hi: u32 = slab_meta.len;
  997|       |    while (lo < hi) {
  998|       |        let mid = (lo + hi) / 2u;
  999|       |        let mid_key = slab[mid].key;
 1000|       |        if (mid_key < key) {
 1001|       |            lo = mid + 1u;
 1002|       |        } else {
 1003|       |            hi = mid;
 1004|       |        }
 1005|       |    }
 1006|       |
 1007|       |    if (lo < slab_meta.len && slab[lo].key == key) {
 1008|       |        slab[lo].value = 0xffffffffu;
 1009|       |    }
 1010|       |}
 1011|       |"#;
 1012|       |
 1013|       |const BULK_SORT_WGSL: &str = r#"
 1014|       |struct KvEntry {
 1015|       |    key: u32,
 1016|       |    value: u32,
 1017|       |};
 1018|       |
 1019|       |struct SortParams {
 1020|       |    k: u32,
 1021|       |    j: u32,
 1022|       |    len: u32,
 1023|       |    _pad: u32,
 1024|       |};
 1025|       |
 1026|       |@group(0) @binding(0) var<storage, read_write> data: array<KvEntry>;
 1027|       |@group(0) @binding(1) var<uniform> params: SortParams;
 1028|       |
 1029|       |@compute @workgroup_size(64)
 1030|       |fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
 1031|       |    let i = gid.x;
 1032|       |    if (i >= params.len) {
 1033|       |        return;
 1034|       |    }
 1035|       |
 1036|       |    let ixj = i ^ params.j;
 1037|       |    if (ixj > i && ixj < params.len) {
 1038|       |        let a = data[i];
 1039|       |        let b = data[ixj];
 1040|       |        let ascending = (i & params.k) == 0u;
 1041|       |        if (ascending) {
 1042|       |            if (a.key > b.key) {
 1043|       |                data[i] = b;
 1044|       |                data[ixj] = a;
 1045|       |            }
 1046|       |        } else {
 1047|       |            if (a.key < b.key) {
 1048|       |                data[i] = b;
 1049|       |                data[ixj] = a;
 1050|       |            }
 1051|       |        }
 1052|       |    }
 1053|       |}
 1054|       |"#;
 1055|       |
 1056|       |const BULK_DEDUP_WGSL: &str = r#"
 1057|       |struct KvEntry {
 1058|       |    key: u32,
 1059|       |    value: u32,
 1060|       |};
 1061|       |
 1062|       |struct DedupParams {
 1063|       |    len: u32,
 1064|       |    _pad0: u32,
 1065|       |    _pad1: u32,
 1066|       |    _pad2: u32,
 1067|       |};
 1068|       |
 1069|       |struct MergeMeta {
 1070|       |    len: u32,
 1071|       |    _pad0: u32,
 1072|       |    _pad1: u32,
 1073|       |    _pad2: u32,
 1074|       |};
 1075|       |
 1076|       |@group(0) @binding(0) var<storage, read_write> data: array<KvEntry>;
 1077|       |@group(0) @binding(1) var<uniform> params: DedupParams;
 1078|       |@group(0) @binding(2) var<storage, read_write> dedup_meta: MergeMeta;
 1079|       |
 1080|       |@compute @workgroup_size(1)
 1081|       |fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
 1082|       |    if (gid.x > 0u) {
 1083|       |        return;
 1084|       |    }
 1085|       |
 1086|       |    let len = params.len;
 1087|       |    if (len == 0u) {
 1088|       |        dedup_meta.len = 0u;
 1089|       |        return;
 1090|       |    }
 1091|       |
 1092|       |    var write_idx: u32 = 0u;
 1093|       |    var prev_key: u32 = 0u;
 1094|       |    var i: u32 = 0u;
 1095|       |    while (i < len) {
 1096|       |        let entry = data[i];
 1097|       |        if (write_idx == 0u) {
 1098|       |            data[write_idx] = entry;
 1099|       |            prev_key = entry.key;
 1100|       |            write_idx = 1u;
 1101|       |        } else {
 1102|       |            if (entry.key == prev_key) {
 1103|       |                data[write_idx - 1u] = entry;
 1104|       |            } else {
 1105|       |                data[write_idx] = entry;
 1106|       |                prev_key = entry.key;
 1107|       |                write_idx = write_idx + 1u;
 1108|       |            }
 1109|       |        }
 1110|       |        i = i + 1u;
 1111|       |    }
 1112|       |
 1113|       |    dedup_meta.len = write_idx;
 1114|       |}
 1115|       |"#;
 1116|       |
 1117|       |const BULK_MERGE_WGSL: &str = r#"
 1118|       |struct KvEntry {
 1119|       |    key: u32,
 1120|       |    value: u32,
 1121|       |};
 1122|       |
 1123|       |struct SlabMeta {
 1124|       |    len: u32,
 1125|       |    capacity: u32,
 1126|       |    _pad0: u32,
 1127|       |    _pad1: u32,
 1128|       |};
 1129|       |
 1130|       |struct InputMeta {
 1131|       |    len: u32,
 1132|       |    _pad0: u32,
 1133|       |    _pad1: u32,
 1134|       |    _pad2: u32,
 1135|       |};
 1136|       |
 1137|       |struct MergeMeta {
 1138|       |    len: u32,
 1139|       |    _pad0: u32,
 1140|       |    _pad1: u32,
 1141|       |    _pad2: u32,
 1142|       |};
 1143|       |
 1144|       |@group(0) @binding(0) var<storage, read> slab: array<KvEntry>;
 1145|       |@group(0) @binding(1) var<storage, read> input: array<KvEntry>;
 1146|       |@group(0) @binding(2) var<storage, read_write> output: array<KvEntry>;
 1147|       |@group(0) @binding(3) var<uniform> slab_meta: SlabMeta;
 1148|       |@group(0) @binding(4) var<uniform> input_meta: InputMeta;
 1149|       |@group(0) @binding(5) var<storage, read_write> merge_meta: MergeMeta;
 1150|       |
 1151|       |fn merge_partition(k: u32, slab_len: u32, input_len: u32) -> vec2<u32> {
 1152|       |    var i_low: u32 = 0u;
 1153|       |    if (k > input_len) {
 1154|       |        i_low = k - input_len;
 1155|       |    }
 1156|       |    var i_high: u32 = k;
 1157|       |    if (i_high > slab_len) {
 1158|       |        i_high = slab_len;
 1159|       |    }
 1160|       |
 1161|       |    var i: u32 = i_high;
 1162|       |    var j: u32 = k - i;
 1163|       |    loop {
 1164|       |        let move_left = i > 0u && j < input_len && slab[i - 1u].key >= input[j].key;
 1165|       |        let move_right = j > 0u && i < slab_len && input[j - 1u].key > slab[i].key;
 1166|       |        if (move_left) {
 1167|       |            i_high = i - 1u;
 1168|       |            i = (i_low + i_high) / 2u;
 1169|       |            j = k - i;
 1170|       |            continue;
 1171|       |        }
 1172|       |        if (move_right) {
 1173|       |            i_low = i + 1u;
 1174|       |            i = (i_low + i_high + 1u) / 2u;
 1175|       |            j = k - i;
 1176|       |            continue;
 1177|       |        }
 1178|       |        break;
 1179|       |    }
 1180|       |    return vec2<u32>(i, j);
 1181|       |}
 1182|       |
 1183|       |@compute @workgroup_size(64)
 1184|       |fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
 1185|       |    let slab_len = slab_meta.len;
 1186|       |    let input_len = input_meta.len;
 1187|       |    let total_len = slab_len + input_len;
 1188|       |    if (total_len == 0u) {
 1189|       |        if (gid.x == 0u) {
 1190|       |            merge_meta.len = 0u;
 1191|       |        }
 1192|       |        return;
 1193|       |    }
 1194|       |
 1195|       |    let chunk_size: u32 = 256u;
 1196|       |    let chunk_count = (total_len + chunk_size - 1u) / chunk_size;
 1197|       |    let chunk_index = gid.x;
 1198|       |    if (chunk_index >= chunk_count) {
 1199|       |        return;
 1200|       |    }
 1201|       |
 1202|       |    let k0 = chunk_index * chunk_size;
 1203|       |    var k1 = k0 + chunk_size;
 1204|       |    if (k1 > total_len) {
 1205|       |        k1 = total_len;
 1206|       |    }
 1207|       |
 1208|       |    let start = merge_partition(k0, slab_len, input_len);
 1209|       |    let end = merge_partition(k1, slab_len, input_len);
 1210|       |
 1211|       |    var i = start.x;
 1212|       |    var j = start.y;
 1213|       |    var k = k0;
 1214|       |
 1215|       |    while (k < k1) {
 1216|       |        if (i < end.x && j < end.y) {
 1217|       |            let a = slab[i];
 1218|       |            let b = input[j];
 1219|       |            if (a.key < b.key) {
 1220|       |                output[k] = a;
 1221|       |                i = i + 1u;
 1222|       |            } else {
 1223|       |                output[k] = b;
 1224|       |                j = j + 1u;
 1225|       |            }
 1226|       |        } else if (i < end.x) {
 1227|       |            output[k] = slab[i];
 1228|       |            i = i + 1u;
 1229|       |        } else {
 1230|       |            output[k] = input[j];
 1231|       |            j = j + 1u;
 1232|       |        }
 1233|       |        k = k + 1u;
 1234|       |    }
 1235|       |
 1236|       |    if (chunk_index == 0u) {
 1237|       |        merge_meta.len = total_len;
 1238|       |    }
 1239|       |}
 1240|       |"#;
